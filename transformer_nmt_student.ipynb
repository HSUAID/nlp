{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Transformer翻译项目"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "toc": true
   },
   "source": [
    "<h1>Table of Contents<span class=\"tocSkip\"></span></h1>\n",
    "<div class=\"toc\"><ul class=\"toc-item\"><li><span><a href=\"#一.-建立Transformer模型的直观认识\" data-toc-modified-id=\"一.-建立Transformer模型的直观认识-1\">一. 建立Transformer模型的直观认识</a></span></li><li><span><a href=\"#二.-编码器部分（Encoder）\" data-toc-modified-id=\"二.-编码器部分（Encoder）-2\">二. 编码器部分（Encoder）</a></span><ul class=\"toc-item\"><li><span><a href=\"#0.-先准备好输入的数据\" data-toc-modified-id=\"0.-先准备好输入的数据-2.1\">0. 先准备好输入的数据</a></span></li><li><span><a href=\"#1.-positional-encoding（即位置嵌入或位置编码）\" data-toc-modified-id=\"1.-positional-encoding（即位置嵌入或位置编码）-2.2\">1. positional encoding（即位置嵌入或位置编码）</a></span></li><li><span><a href=\"#2.-self-attention（自注意力机制）\" data-toc-modified-id=\"2.-self-attention（自注意力机制）-2.3\">2. self attention（自注意力机制）</a></span></li><li><span><a href=\"#3.-Attention-Mask\" data-toc-modified-id=\"3.-Attention-Mask-2.4\">3. Attention Mask</a></span></li><li><span><a href=\"#4.-Layer-Normalization-和残差连接\" data-toc-modified-id=\"4.-Layer-Normalization-和残差连接-2.5\">4. Layer Normalization 和残差连接</a></span></li><li><span><a href=\"#5.-Transformer-Encoder-整体结构\" data-toc-modified-id=\"5.-Transformer-Encoder-整体结构-2.6\">5. Transformer Encoder 整体结构</a></span></li></ul></li><li><span><a href=\"#三.-解码器部分（Decoder）\" data-toc-modified-id=\"三.-解码器部分（Decoder）-3\">三. 解码器部分（Decoder）</a></span></li><li><span><a href=\"#四.-Transformer模型\" data-toc-modified-id=\"四.-Transformer模型-4\">四. Transformer模型</a></span></li><li><span><a href=\"#五.-模型训练\" data-toc-modified-id=\"五.-模型训练-5\">五. 模型训练</a></span></li><li><span><a href=\"#六.-模型预测\" data-toc-modified-id=\"六.-模型预测-6\">六. 模型预测</a></span></li></ul></div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这份notebook当中，我们会(尽可能)实现 $Transformer$ 模型来完成翻译任务。  \n",
    "（参考论文：$Attention\\; Is\\; All\\; You\\; Need$   https://arxiv.org/pdf/1706.03762.pdf ）\n",
    "> **其中的`TODO`内容即为待完成的部分**\n",
    "\n",
    "我们的数据集非常小，只有一万多个句子的训练数据，从结果来看训练出来的模型在测试集上的表现其实已经算还可以了。  \n",
    "如果想得到更好的效果，则需要更大的数据量并进行更多的训练迭代次数，感兴趣（并且有硬件条件）的同学可以进行尝试。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 一. 建立Transformer模型的直观认识\n",
    "  \n",
    "首先来说一下**Transformer**和**LSTM**的最大区别，就是LSTM的训练是迭代（自回归）的，是一个接一个字的来，当前这个字过完LSTM单元，才可以进下一个字，而 $Transformer$ 的训练是并行了，就是所有字是全部同时训练的，这样就大大加快了计算效率，$Transformer$ 使用了位置嵌入$(positional \\ encoding)$来理解语言的顺序，使用自注意力机制和全连接层来进行计算，这些后面都会详细讲解。   \n",
    "  \n",
    "$Transformer$ 模型主要分为**两大部分**，分别是**编码器（$Encoder$）**和**解码器（$Decoder$）**：  \n",
    "- **编码器（$Encoder$）**负责把自然语言序列映射成为**隐藏层**(下图中**第2步**用九宫格比喻的部分)，含有自然语言序列的数学表达\n",
    "- **解码器（$Decoder$）**再把隐藏层映射为自然语言序列，从而使我们可以解决各种问题，如情感分类、命名实体识别、语义关系抽取、摘要生成、机器翻译等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/intuition.jpg\", width=650>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<img src=\"./imgs/intuition.jpg\", width=660>\n",
       "<img src=\"./imgs/transformer.png\", width=660>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%%html\n",
    "\n",
    "<img src=\"./imgs/intuition.jpg\", width=660>\n",
    "<img src=\"./imgs/transformer.png\", width=660>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 二. 编码器部分（Encoder）\n",
    "  \n",
    "  \n",
    "我们会**重点介绍编码器的结构**，因为理解了编码器中的结构, 理解解码器就非常简单了。而且我们用编码器就能够完成一些自然语言处理中比较主流的任务, 如情感分类, 语义关系分析, 命名实体识别等。  \n",
    "  \n",
    "**编码器（$Encoder$）**部分, 即把**自然语言序列映射为隐藏层的数学表达的过程**。  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** 以下为一个Transformer Encoder Block结构示意图**\n",
    "> 注意: 为方便查看, 下面各部分的内容分别对应着图中第1, 2, 3, 4个方框的序号："
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/encoder.jpg\", width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. 先准备好输入的数据"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import math\n",
    "import copy\n",
    "import time\n",
    "import numpy as np\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.nn.functional as F\n",
    "import tensorflow as tf\n",
    "import tensorflow.nn as nn\n",
    "from nltk import word_tokenize\n",
    "from collections import Counter\n",
    "# from torch.autograd import Variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化参数设置\n",
    "UNK = 0  # 未登录词的标识符对应的词典id\n",
    "PAD = 1  # padding占位符对应的词典id\n",
    "BATCH_SIZE = 64  # 每批次训练数据数量\n",
    "EPOCHS = 200  # 训练轮数\n",
    "LAYERS = 6  # transformer中堆叠的encoder和decoder block层数\n",
    "H_NUM = 8  # multihead attention hidden个数\n",
    "D_MODEL = 256  # embedding维数\n",
    "D_FF = 1024  # feed forward第一个全连接层维数\n",
    "DROPOUT = 0.1  # dropout比例\n",
    "MAX_LENGTH = 60  # 最大句子长度\n",
    "\n",
    "TRAIN_FILE = 'nmt/en-cn/train.txt'  # 训练集数据文件\n",
    "DEV_FILE = \"nmt/en-cn/dev.txt\"  # 验证(开发)集数据文件\n",
    "# SAVE_FILE = 'save/model.pt'  # 模型保存路径(注意如当前目录无save文件夹需要自己创建)\n",
    "# DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np  \n",
    "np.set_printoptions(threshold=np.inf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.core.interactiveshell import InteractiveShell \n",
    "InteractiveShell.ast_node_interactivity = \"all\" "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [],
   "source": [
    "def seq_padding(X, padding=0):\n",
    "    \"\"\"\n",
    "    对一个batch批次(以单词id表示)的数据进行padding填充对齐长度\n",
    "    \"\"\"\n",
    "    # 计算该批次数据各条数据句子长度\n",
    "    L = [len(x) for x in X]\n",
    "    # 获取该批次数据最大句子长度\n",
    "    ML = max(L)\n",
    "    # 对X中各条数据x进行遍历，如果长度短于该批次数据最大长度ML，则以padding id填充缺失长度ML-len(x)\n",
    "    # （注意这里默认padding id是0，相当于是拿<UNK>来做了padding）\n",
    "    return np.array([\n",
    "        np.concatenate([x, [padding] * (ML - len(x))]) if len(x) < ML else x for x in X\n",
    "    ])\n",
    "\n",
    "\n",
    "class PrepareData:\n",
    "    def __init__(self, train_file, dev_file):\n",
    "        # 读取数据 并分词\n",
    "        self.train_en, self.train_cn = self.load_data(train_file)\n",
    "        self.dev_en, self.dev_cn = self.load_data(dev_file)\n",
    "\n",
    "        # 构建单词表\n",
    "        self.en_word_dict, self.en_total_words, self.en_index_dict = self.build_dict(self.train_en)\n",
    "        self.cn_word_dict, self.cn_total_words, self.cn_index_dict = self.build_dict(self.train_cn)\n",
    "\n",
    "        # id化\n",
    "        self.train_en, self.train_cn = self.wordToID(self.train_en, self.train_cn, self.en_word_dict, self.cn_word_dict)\n",
    "        self.dev_en, self.dev_cn = self.wordToID(self.dev_en, self.dev_cn, self.en_word_dict, self.cn_word_dict)\n",
    "\n",
    "        # 划分batch + padding + mask\n",
    "        self.train_data = self.splitBatch(self.train_en, self.train_cn, BATCH_SIZE)\n",
    "        self.dev_data = self.splitBatch(self.dev_en, self.dev_cn, BATCH_SIZE)\n",
    "\n",
    "    def load_data(self, path):\n",
    "        \"\"\"\n",
    "        读取翻译前(英文)和翻译后(中文)的数据文件\n",
    "        每条数据都进行分词，然后构建成包含起始符(BOS)和终止符(EOS)的单词(中文为字符)列表\n",
    "        形式如：en = [['BOS', 'i', 'love', 'you', 'EOS'], ['BOS', 'me', 'too', 'EOS'], ...]\n",
    "                cn = [['BOS', '我', '爱', '你', 'EOS'], ['BOS', '我', '也', '是', 'EOS'], ...]\n",
    "        \"\"\"\n",
    "        en = []\n",
    "        cn = []\n",
    "        with open(path, 'r', encoding='utf-8') as f:\n",
    "            for line in f:\n",
    "                line = line.strip().split('\\t')\n",
    "\n",
    "                en.append([\"BOS\"] + word_tokenize(line[0].lower()) + [\"EOS\"])\n",
    "                cn.append([\"BOS\"] + word_tokenize(\" \".join([w for w in line[1]])) + [\"EOS\"])\n",
    "#         print(cn[63])\n",
    "        return en, cn\n",
    "    \n",
    "    def build_dict(self, sentences, max_words=50000):\n",
    "        \"\"\"\n",
    "        传入load_data构造的分词后的列表数据\n",
    "        构建词典(key为单词，value为id值)\n",
    "        \"\"\"\n",
    "        # 对数据中所有单词进行计数\n",
    "        word_count = Counter()\n",
    "\n",
    "        for sentence in sentences:\n",
    "            for s in sentence:\n",
    "                word_count[s] += 1\n",
    "        # 只保留最高频的前max_words数的单词构建词典\n",
    "        # 并添加上UNK和PAD两个单词，对应id已经初始化设置过\n",
    "        ls = word_count.most_common(max_words)\n",
    "        # 统计词典的总词数\n",
    "        total_words = len(ls) + 2\n",
    "\n",
    "        word_dict = {w[0]: index + 2 for index, w in enumerate(ls)}\n",
    "        word_dict['UNK'] = UNK\n",
    "        word_dict['PAD'] = PAD\n",
    "        # 再构建一个反向的词典，供id转单词使用\n",
    "        index_dict = {v: k for k, v in word_dict.items()}\n",
    "\n",
    "        return word_dict, total_words, index_dict\n",
    "\n",
    "    def wordToID(self, en, cn, en_dict, cn_dict, sort=True):\n",
    "        \"\"\"\n",
    "        该方法可以将翻译前(英文)数据和翻译后(中文)数据的单词列表表示的数据\n",
    "        均转为id列表表示的数据\n",
    "        如果sort参数设置为True，则会以翻译前(英文)的句子(单词数)长度排序\n",
    "        以便后续分batch做padding时，同批次各句子需要padding的长度相近减少padding量\n",
    "        \"\"\"\n",
    "        # 计算英文数据条数\n",
    "        length = len(en)\n",
    "        # 将翻译前(英文)数据和翻译后(中文)数据都转换为id表示的形式\n",
    "        out_en_ids = [[en_dict.get(w, 0) for w in sent] for sent in en]\n",
    "        out_cn_ids = [[cn_dict.get(w, 0) for w in sent] for sent in cn]\n",
    "\n",
    "        # 构建一个按照句子长度排序的函数\n",
    "        def len_argsort(seq):\n",
    "            \"\"\"\n",
    "            传入一系列句子数据(分好词的列表形式)，\n",
    "            按照句子长度排序后，返回排序后原来各句子在数据中的索引下标\n",
    "            \"\"\"\n",
    "            return sorted(range(len(seq)), key=lambda x: len(seq[x]))\n",
    "\n",
    "        # 把中文和英文按照同样的顺序排序\n",
    "        if sort:\n",
    "            # 以英文句子长度排序的(句子下标)顺序为基准\n",
    "            sorted_index = len_argsort(out_en_ids)\n",
    "#             print(sorted_index[0])\n",
    "#             print(out_cn_ids[sorted_index[0]])\n",
    "            # 对翻译前(英文)数据和翻译后(中文)数据都按此基准进行排序\n",
    "            out_en_ids = [out_en_ids[i] for i in sorted_index]\n",
    "            out_cn_ids = [out_cn_ids[i] for i in sorted_index]\n",
    "            \n",
    "        return out_en_ids, out_cn_ids\n",
    "\n",
    "    def splitBatch(self, en, cn, batch_size):\n",
    "        \"\"\"\n",
    "        将以单词id列表表示的翻译前(英文)数据和翻译后(中文)数据\n",
    "        按照指定的batch_size进行划分\n",
    "        如果shuffle参数为True，则会对这些batch数据顺序进行随机打乱\n",
    "        \"\"\"\n",
    "        # 在按数据长度生成的各条数据下标列表[0, 1, ..., len(en)-1]中\n",
    "        # 每隔指定长度(batch_size)取一个下标作为后续生成batch的起始下标\n",
    "        # 如果shuffle参数为True，则将这些各batch起始下标打乱\n",
    "        # 存放各个batch批次的句子数据索引下标\n",
    "            # 注意，起始下标最大的那个batch可能会超出数据大小\n",
    "            # 因此要限定其终止下标不能超过数据大小\n",
    "        \n",
    "        # 按各batch批次的句子数据索引下标，构建实际的单词id列表表示的各batch句子数据\n",
    "#         batches = []\n",
    "#         for batch_index in batch_indexs:\n",
    "#             # 按当前batch的各句子下标(数组批量索引)提取对应的单词id列表句子表示数据\n",
    "#             batch_en = [en[index] for index in batch_index]  \n",
    "#             batch_cn = [cn[index] for index in batch_index]\n",
    "#             # 对当前batch的各个句子都进行padding对齐长度\n",
    "#             # 维度为：batch数量×batch_size×每个batch最大句子长度\n",
    "#             batch_cn = seq_padding(batch_cn)\n",
    "#             batch_en = seq_padding(batch_en)\n",
    "#             # 将当前batch的英文和中文数据添加到存放所有batch数据的列表中\n",
    "#             batches.append(Batch(batch_en, batch_cn))\n",
    "        X = np.zeros([len(en), MAX_LENGTH], np.int32)\n",
    "        Y = np.zeros([len(cn), MAX_LENGTH], np.int32)\n",
    "        for i, (x, y) in enumerate(zip(en, cn)):\n",
    "            X[i] = np.lib.pad(x, [0, MAX_LENGTH - len(x)], 'constant', constant_values=(0, 0))\n",
    "            Y[i] = np.lib.pad(y, [0, MAX_LENGTH - len(y)], 'constant', constant_values=(0, 0))\n",
    "            num_batch = len(X) // BATCH_SIZE\n",
    "        # convert to tensor\n",
    "        X = tf.convert_to_tensor(X, tf.int32)\n",
    "        Y = tf.convert_to_tensor(Y, tf.int32)\n",
    "        # 构建数据集，打散，批量，并丢掉最后一个不够batch_size 的batch\n",
    "        input_queues = tf.data.Dataset.from_tensor_slices((X, Y))\n",
    "        input_queues = input_queues.shuffle(1000).batch(BATCH_SIZE, drop_remainder=True)\n",
    "#         input_queues = input_queues.batch(BATCH_SIZE, drop_remainder=True)      \n",
    "        return input_queues  #  shape=([batch_size, max_seq_len], [batch_size, max_seq_len])          \n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PrepareData(TRAIN_FILE, DEV_FILE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意，上述预处理中使用的 $Batch$ 类在后面的 $Encoder$ 内容的 $Attention\\ Mask$ 部分定义"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Embeddings**  \n",
    "  \n",
    "与其他序列传导模型类似，我们使用learned embeddings将输入标记和输出标记转换为维度 $d_{model}$ 的向量。我们还使用通常学习的线性变换和softmax函数将 $Decoder$（解码器）的输出转换为预测的下一个标签的概率。  \n",
    "  \n",
    "在我们的模型中，我们在两个embedding层和pre-softmax线性变换层之间共享相同的权重矩阵。在其中的embedding层，我们会将这些权重乘以 $\\sqrt{d_{model}}$ 。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Embeddings(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, vocab):\n",
    "        super(Embeddings, self).__init__()\n",
    "        # Embedding层\n",
    "        self.lut =  tf.get_variable('lut',\n",
    "                                       dtype=tf.float32,\n",
    "                                       shape=[vocab, d_model],\n",
    "                                       initializer=tf.contrib.layers.xavier_initializer())\n",
    "#         self.lut = nn.Embedding(vocab, d_model)\n",
    "        # Embedding维数\n",
    "        self.d_model = d_model\n",
    "\n",
    "    def call(self, x):\n",
    "        # 返回x对应的embedding矩阵（需要乘以math.sqrt(d_model)）\n",
    "        return nn.embedding_lookup(self.lut, X) * math.sqrt(self.d_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "数据全部处理完成，现在我们开始理解和构建 $Transformer$ 模型"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. positional encoding（即位置嵌入或位置编码）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "由于 $Transformer$ 模型**没有**循环神经网络的迭代操作，所以我们必须提供每个字的位置信息给 $Transformer$，才能识别出语言中的顺序关系。   \n",
    "  \n",
    "因此，我们定义一个位置嵌入的概念，也就是$positional \\ encoding$，位置嵌入的维度为$[max \\ sequence \\ length，\\ embedding \\ dimension]$，嵌入的维度同词向量的维度，$max \\ sequence \\ length$属于超参数，指的是限定的最大单个句长。   \n",
    "  \n",
    "注意，我们一般以字为单位训练transformer模型，也就是说我们不用分词了，首先我们要初始化字向量为$[vocab \\ size，\\ embedding \\ dimension]$，$vocab \\ size$为总共的字库数量，$embedding \\ dimension$为字向量的维度，也是每个字的数学表达。    \n",
    "  \n",
    "在论文 **attention is all you need**（ https://arxiv.org/pdf/1706.03762.pdf ）中使用了$sine$和$cosine$函数的线性变换来提供给模型位置信息：   \n",
    "  \n",
    "$$PE_{(pos,2i)} = sin(pos / 10000^{2i/d_{\\text{model}}}) \\quad \\quad PE_{(pos,2i+1)} = cos(pos / 10000^{2i/d_{\\text{model}}})\\tag{eq.1}$$  \n",
    "  \n",
    "上式中$pos$指的是句中字的位置，取值范围是$[0, \\ max \\ sequence \\ length)$，$i$指的是词向量的维度，取值范围是$[0, \\ embedding \\ dimension)$，上面有$sin$和$cos$一组公式，也就是对应着$embedding \\ dimension$维度的一组奇数和偶数的序号的维度，例如$0, 1$一组，$2, 3$一组，分别用上面的$sin$和$cos$函数做处理，从而产生不同的周期性变化，而位置嵌入在$embedding \\ dimension$维度上随着维度序号增大，周期变化会越来越慢，而产生一种包含位置信息的纹理，就像论文原文中第六页讲的，位置嵌入函数的周期从$2 \\pi$到$10000 * 2 \\pi$变化，而每一个位置在$embedding \\ dimension$维度上都会得到不同周期的$sin$和$cos$函数的取值组合，从而产生独一的纹理位置信息，模型从而学到位置之间的依赖关系和自然语言的时序特性。   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'return' outside function (<ipython-input-10-526c2fe1d488>, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;36m  File \u001b[1;32m\"<ipython-input-10-526c2fe1d488>\"\u001b[1;36m, line \u001b[1;32m1\u001b[0m\n\u001b[1;33m    return position / np.power(10000., 2. * (i // 2.) / np.float(d_model))\u001b[0m\n\u001b[1;37m                                                                          ^\u001b[0m\n\u001b[1;31mSyntaxError\u001b[0m\u001b[1;31m:\u001b[0m 'return' outside function\n"
     ]
    }
   ],
   "source": [
    "        return position / np.power(10000., 2. * (i // 2.) / np.float(d_model))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 导入依赖库\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def positional_encoding(pos, d_model):\n",
    "    '''\n",
    "    :param pos: 词在句子中的位置，句子上的维族；（i是d_model上的维度）\n",
    "    :param d_model: 隐状态的维度，相当于num_units\n",
    "    :return: 位置编码 shape=[1, position_num, d_model], 其中第一个维度是为了匹配batch_size\n",
    "    '''\n",
    "    def get_angles(position, i):\n",
    "        # 这里的i相当于公式里面的2i或2i+1\n",
    "        # 返回shape=[position_num, d_model]\n",
    "        return position / np.power(10000., 2. * (i // 2.) / np.float(d_model))\n",
    "\n",
    "    angle_rates = get_angles(np.arange(pos)[:, np.newaxis],\n",
    "                             np.arange(d_model)[np.newaxis, :])\n",
    "    # 2i位置使用sin编码，2i+1位置使用cos编码\n",
    "    pe_sin = np.sin(angle_rates[:, 0::2])\n",
    "    pe_cos = np.cos(angle_rates[:, 1::2])\n",
    "    pos_encoding = np.concatenate([pe_sin, pe_cos], axis=-1)\n",
    "    pos_encoding = tf.cast(pos_encoding[np.newaxis, ...], tf.float32)\n",
    "    return pos_encoding\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 50, 512)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.collections.QuadMesh at 0x1d70d5bdac8>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 0, 'Depth')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(0, 512)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "Text(0, 0.5, 'Position')"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "<matplotlib.colorbar.Colorbar at 0x1d7064f8e88>"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# 演示positional_encoding\n",
    "pos_encoding = positional_encoding(50, 512)\n",
    "print(pos_encoding.shape)\n",
    "plt.pcolormesh(pos_encoding[0], cmap='RdBu')\n",
    "plt.xlabel('Depth')\n",
    "plt.xlim((0, 512))\n",
    "plt.ylabel('Position')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "可见，这里首先是按照最大长度max_len生成一个位置，而后根据公式计算出所有的向量，在forward函数中根据长度取用即可，非常方便。  \n",
    "  \n",
    "> 注意要设置requires_grad=False，因其不参与训练。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "下面画一下位置嵌入，可见纵向观察，随着$embedding \\ dimension$增大，位置嵌入函数呈现不同的周期变化。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. self attention（自注意力机制）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/attention_0.jpg\", width=600>\n",
    "<img src=\"./imgs/attention_1.jpg\", width=600>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**除以$\\sqrt{d_k}$的解释**  \n",
    "  \n",
    "假设 $q$ 和 $k$ 是独立的随机变量，平均值为 0，方差 1，这样他们的点积后形成的注意力矩阵为 $q⋅k=\\sum_{i=1}^{d_k}{q_i k_i}$，均值为 0 但方差放大为 $d_k$ 。为了抵消这种影响，我们用$\\sqrt{d_k}$来缩放点积，可以使得Softmax归一化时结果更稳定（不至于点积后得到注意力矩阵的值差别太大），以便反向传播时获取平衡的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*************** 第一部分: Scaled dot-product attention ***************'"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''*************** 第一部分: Scaled dot-product attention ***************'''\n",
    "def scaled_dot_product_attention(q, k, v, mask):\n",
    "    '''attention(Q, K, V) = softmax(Q * K^T / sqrt(dk)) * V'''\n",
    "#     tf.print(mask)\n",
    "    # query 和 Key相乘\n",
    "    matmul_qk = tf.matmul(q, k, transpose_b=True)\n",
    "#     tf.print(matmul_qk)\n",
    "    # 使用dk进行缩放\n",
    "    dk = tf.cast(tf.shape(q)[-1], tf.float32)\n",
    "    scaled_attention =matmul_qk / tf.math.sqrt(dk)\n",
    "    # 掩码mask\n",
    "    if mask is not None:\n",
    "        # 这里将mask的token乘以-1e-9，这样与attention相加后，mask的位置经过softmax后就为0\n",
    "        # padding位置 mask=1\n",
    "        scaled_attention += mask * -1e9\n",
    "#         tf.print(scaled_attention) \n",
    "    # 通过softmax获取attention权重, mask部分softmax后为0\n",
    "    attention_weights = tf.nn.softmax(scaled_attention)  # shape=[batch_size, seq_len_q, seq_len_k]\n",
    "#     print('attention_weights000000000000000000')\n",
    "#     tf.print(attention_weights)    \n",
    "    # 乘以value\n",
    "    outputs = tf.matmul(attention_weights, v)  # shape=[batch_size, seq_len_q, depth]\n",
    "    return outputs, attention_weights\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-999999998.0"
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a = 2\n",
    "a += 1 * -1e9\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'*************** 第二部分: Multi-Head Attention ***************'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "'\\nmulti-head attention包含3部分： - 线性层与分头 - 缩放点积注意力 - 头连接 - 末尾线性层\\n每个多头注意块有三个输入; Q（查询），K（密钥），V（值）。 它们通过第一层线性层并分成多个头。\\n注意:点积注意力时需要使用mask， 多头输出需要使用tf.transpose调整各维度。\\nQ，K和V不是一个单独的注意头，而是分成多个头，因为它允许模型共同参与来自不同表征空间的不同信息。\\n在拆分之后，每个头部具有降低的维度，总计算成本与具有全维度的单个头部注意力相同。\\n'"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''*************** 第二部分: Multi-Head Attention ***************'''\n",
    "'''\n",
    "multi-head attention包含3部分： - 线性层与分头 - 缩放点积注意力 - 头连接 - 末尾线性层\n",
    "每个多头注意块有三个输入; Q（查询），K（密钥），V（值）。 它们通过第一层线性层并分成多个头。\n",
    "注意:点积注意力时需要使用mask， 多头输出需要使用tf.transpose调整各维度。\n",
    "Q，K和V不是一个单独的注意头，而是分成多个头，因为它允许模型共同参与来自不同表征空间的不同信息。\n",
    "在拆分之后，每个头部具有降低的维度，总计算成本与具有全维度的单个头部注意力相同。\n",
    "'''\n",
    "class MultiHeadAttention(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads):\n",
    "        super(MultiHeadAttention, self).__init__()\n",
    "        self.num_heads = num_heads\n",
    "        self.d_model = d_model\n",
    "        # d_model必须可以正确分成多个头\n",
    "        assert d_model % num_heads == 0\n",
    "        # 分头之后维度\n",
    "        self.depth = d_model // num_heads\n",
    "        self.wq = tf.keras.layers.Dense(d_model)\n",
    "        self.wk = tf.keras.layers.Dense(d_model)\n",
    "        self.wv = tf.keras.layers.Dense(d_model)\n",
    "        self.dense = tf.keras.layers.Dense(d_model)\n",
    "\n",
    "    def split_heads(self, x, batch_size):\n",
    "        # 分头，将头个数的维度，放到seq_len前面 x输入shape=[batch_size, seq_len, d_model]\n",
    "        x = tf.reshape(x, [batch_size, -1, self.num_heads, self.depth])\n",
    "        return tf.transpose(x, perm=[0, 2, 1, 3])\n",
    "\n",
    "    def call(self, q, k, v, mask):\n",
    "        batch_size = tf.shape(q)[0]\n",
    "        # 分头前的前向网络，根据q,k,v的输入，计算Q, K, V语义\n",
    "        q = self.wq(q)  # shape=[batch_size, seq_len_q, d_model]\n",
    "        k = self.wq(k)\n",
    "        v = self.wq(v)\n",
    "        # 分头\n",
    "        q = self.split_heads(q, batch_size)  # shape=[batch_size, num_heads, seq_len_q, depth]\n",
    "        k = self.split_heads(k, batch_size)\n",
    "        v = self.split_heads(v, batch_size)\n",
    "        # 通过缩放点积注意力层\n",
    "        # scaled_attention shape=[batch_size, num_heads, seq_len_q, depth]\n",
    "        # attention_weights shape=[batch_size, num_heads, seq_len_q, seq_len_k]\n",
    "#         print('where')        \n",
    "        scaled_attention, attention_weights = scaled_dot_product_attention(q, k, v, mask)\n",
    "        # 把多头维度后移\n",
    "        scaled_attention = tf.transpose(scaled_attention, perm=[0, 2, 1, 3]) # shape=[batch_size, seq_len_q, num_heads, depth]\n",
    "        # 把多头合并\n",
    "        concat_attention = tf.reshape(scaled_attention, (batch_size, -1, self.d_model)) # shape=[batch_size, seq_len_q, d_model]\n",
    "        # 全连接重塑\n",
    "        output = self.dense(concat_attention)\n",
    "        return output, attention_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(1, 60, 512) (1, 8, 60, 60)\n"
     ]
    }
   ],
   "source": [
    "# 测试multi-head attention\n",
    "temp_mha = MultiHeadAttention(d_model=512, num_heads=8)\n",
    "y = tf.random.uniform((1, 60, 512))\n",
    "output, att = temp_mha(y, y, y, None)\n",
    "print(output.shape, att.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参数里面的 $h$ 和 $d_{model}$ 分别表示注意力头的个数，以及模型的隐层单元数。  \n",
    "  \n",
    "另外在 $\\_\\_init\\_\\_$ 函数中，我们定义了 $self.linears = clones(nn.Linear(d_{model},\\; d_{model}),\\; 4),\\; clone(x,\\; N)$ 即为深拷贝N份，这里定义了4个全连接函数，实际上是3+1，其中的3个分别是Q、K和V的变换矩阵，最后一个是用于最后将 $h$ 个多头注意力矩阵concat之后进行变换的矩阵。  \n",
    "  \n",
    "在 $forward$ 函数中，是首先将 $query$、$key$ 和 $value$ 进行相应的变换，然后需要经过 $attention$ 这个函数的计算，这个函数实际上就是论文中“Scaled Dot-Product Attention”这个模块的计算"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Attention Mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/attention_mask.jpg\", width=750>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "注意, 在上面$self \\ attention$的计算过程中, 我们通常使用$mini \\ batch$来计算, 也就是一次计算多句话, 也就是$X$的维度是$[batch \\ size, \\ sequence \\ length]$, $sequence \\ length$是句长, 而一个$mini \\ batch$是由多个不等长的句子组成的, 我们就需要按照这个$mini \\ batch$中最大的句长对剩余的句子进行补齐长度, 我们一般用$0$来进行填充, 这个过程叫做$padding$.   \n",
    "  \n",
    "但这时在进行$softmax$的时候就会产生问题, 回顾$softmax$函数$\\sigma (\\mathbf {z} )_{i}={\\frac {e^{z_{i}}}{\\sum _{j=1}^{K}e^{z_{j}}}}$, $e^0$是1, 是有值的, 这样的话$softmax$中被$padding$的部分就参与了运算, 就等于是让无效的部分参与了运算, 会产生很大隐患, 这时就需要做一个$mask$让这些无效区域不参与运算, 我们一般给无效区域加一个很大的负数的偏置, 也就是:  \n",
    "  \n",
    "$$z_{illegal} = z_{illegal} + bias_{illegal}$$\n",
    "$$bias_{illegal} \\to -\\infty$$\n",
    "$$e^{z_{illegal}} \\to 0 $$  \n",
    "  \n",
    "经过上式的$masking$我们使无效区域经过$softmax$计算之后几乎为$0$, 这样就避免了无效区域参与计算."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在 $Transformer$ 里面，$Encoder$ 和 $Decoder $的 $Attention$ 计算都需要相应的 $Mask$ 处理，但功能却不同。  \n",
    "  \n",
    "在 $Encoder$ 中，就如上述介绍的，$Mask$ 就是为了让那些在一个 $batch$ 中长度较短的序列的 $padding$ 部分不参与 $Attention$ 的计算。因此我们定义一个 $Batch$ 批处理对象，它包含用于训练的 $src$（翻译前）和 $trg$（翻译后）句子，以及构造其中的 $Mask$ 掩码。  \n",
    "  \n",
    "**加了 $Mask$ 的 $Attention$ 原理如图（另附 $Multi\\text{-}Head\\ Attention$ ）：**  \n",
    "> 注意：这里的 $Attention\\ Mask$ 是加在 $Scale$ 和 $Softmax$ 之间  \n",
    "  \n",
    "<img src=\"./imgs/attention_mask2.jpg\", width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Layer Normalization 和残差连接"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1). **LayerNorm**:   \n",
    "  \n",
    "$Layer Normalization$的作用是把神经网络中隐藏层归一为标准正态分布, 也就是 $i.i.d$（独立同分布）, 以起到加快训练速度, 加速收敛的作用:\n",
    "$$\\mu_{i}=\\frac{1}{m} \\sum^{m}_{i=1}x_{ij}$$  \n",
    "  \n",
    "上式中以矩阵的行$(row)$为单位求均值;  \n",
    "  \n",
    "$$\\sigma^{2}_{j}=\\frac{1}{m} \\sum^{m}_{i=1}\n",
    "(x_{ij}-\\mu_{j})^{2}$$  \n",
    "  \n",
    "上式中以矩阵的行$(row)$为单位求方差;  \n",
    "  \n",
    "$$LayerNorm(x)=\\alpha \\odot \\frac{x_{ij}-\\mu_{i}}\n",
    "{\\sqrt{\\sigma^{2}_{i}+\\epsilon}} + \\beta \\tag{eq.5}$$  \n",
    "  \n",
    "然后用**每一行**的**每一个元素**减去**这行的均值**, 再除以**这行的标准差**, 从而得到归一化后的数值, $\\epsilon$是为了防止除$0$;   \n",
    "之后引入两个**可训练参数**$\\alpha, \\ \\beta$来弥补归一化的过程中损失掉的信息, 注意$\\odot$表示元素相乘而不是点积, 我们一般初始化$\\alpha$为全$1$, 而$\\beta$为全$0$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2). **残差连接**:   \n",
    "  \n",
    "我们在上一步得到了经过注意力矩阵加权之后的$V$, 也就是$Attention(Q, \\ K, \\ V)$, 我们对它进行一下转置, 使其和$X_{embedding}$的维度一致, 也就是$[batch \\ size, \\ sequence \\ length, \\ embedding \\ dimension]$, 然后把他们加起来做残差连接, 直接进行元素相加, 因为他们的维度一致:   \n",
    "  \n",
    "$$X_{embedding} + Attention(Q, \\ K, \\ V)$$  \n",
    "  \n",
    "在之后的运算里, 每经过一个模块的运算, 都要把运算之前的值和运算之后的值相加, 从而得到残差连接, 训练的时候可以使梯度直接走捷径反传到最初始层:  \n",
    "  \n",
    "$$X + SubLayer(X) \\tag{eq. 6}$$  \n",
    "  \n",
    "> **注意：这里我们对$SubLayer(X)$一般会进行dropout后再与X连接，即　$X + Dropout(SubLayer(X))$**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LayerNormalization(tf.keras.layers.Layer):\n",
    "    def __init__(self, epsilon=1e-8, **kwargs):\n",
    "        super(LayerNormalization, self).__init__(**kwargs)\n",
    "        self.epsilon = epsilon\n",
    "    def build(self, input_shape):\n",
    "        self.gamma = self.add_weight(name='gamma',\n",
    "                                     shape=input_shape[-1:],\n",
    "                                     initializer=tf.ones_initializer(),\n",
    "                                     trainable=True)\n",
    "        self.beta = self.add_weight(name='beta',\n",
    "                                    shape=input_shape[-1:],\n",
    "                                    initializer=tf.zeros_initializer(),\n",
    "                                    trainable=True)\n",
    "        super(LayerNormalization, self).build(input_shape)\n",
    "    def call(self, x): # x shape=[batch_size, seq_len, d_model]\n",
    "        mean = tf.keras.backend.mean(x, axis=-1, keepdims=True)\n",
    "        std = tf.keras.backend.std(x, axis=-1, keepdims=True)\n",
    "        return self.gamma * (x - mean) / (std + self.epsilon) + self.beta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def point_wise_feed_forward(d_model, diff):\n",
    "    return tf.keras.Sequential([\n",
    "        tf.keras.layers.Dense(diff, activation=tf.nn.relu),\n",
    "        tf.keras.layers.Dense(d_model)\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以上是 $LayerNormalization$ 的实现，其实PyTorch里面已经集成好了nn.LayerNorm，这里实现出来是为了学习其中的原理。而实际中，为了代码简洁，可以直接使用PyTorch里面实现好的函数。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''encoder layer:\n",
    "每个编码层包含以下子层 - Multi-head attention（带掩码） - Point wise feed forward networks\n",
    "每个子层中都有残差连接，并最后通过一个正则化层。残差连接有助于避免深度网络中的梯度消失问题。 \n",
    "每个子层输出是LayerNorm(x + Sublayer(x))，规范化是在d_model维的向量上。Transformer一共有n个编码层。\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EncoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(EncoderLayer, self).__init__()\n",
    "        self.mha = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward(d_model, dff)\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def call(self, inputs, training, mask):\n",
    "        # multi head attention (encoder时Q = K = V)\n",
    "        att_output, _ = self.mha(inputs, inputs, inputs, mask)\n",
    "        att_output = self.dropout1(att_output, training=training)\n",
    "        output1 = self.layernorm1(inputs + att_output)  # shape=[batch_size, seq_len, d_model]\n",
    "        # feed forward network\n",
    "        ffn_output = self.ffn(output1)\n",
    "        ffn_output = self.dropout2(ffn_output, training=training)\n",
    "        output2 = self.layernorm2(output1 + ffn_output)  # shape=[batch_size, seq_len, d_model]\n",
    "        return output2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class SublayerConnection(nn.Module):\n",
    "#     \"\"\"\n",
    "#     SublayerConnection的作用就是把Multi-Head Attention和Feed Forward层连在一起\n",
    "#     只不过每一层输出之后都要先做Layer Norm再残差连接\n",
    "#     \"\"\"\n",
    "#     def __init__(self, size, dropout):\n",
    "#         super(SublayerConnection, self).__init__()\n",
    "#         self.norm = LayerNorm(size)\n",
    "#         self.dropout = nn.Dropout(dropout)\n",
    "\n",
    "#     def forward(self, x, sublayer):\n",
    "#         # TODO: 请利用init中的成员变量实现LayerNorm和残差连接的功能\n",
    "#         # 返回Layer Norm和残差连接后结果\n",
    "#         return "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Transformer Encoder 整体结构"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "经过上面4个步骤, 我们已经基本了解到来$transformer$编码器的主要构成部分, 我们下面用公式把一个$transformer \\ block$的计算过程整理一下:    \n",
    "  \n",
    "1). **字向量与位置编码:**   \n",
    "$$X = EmbeddingLookup(X) + PositionalEncoding \\tag{eq.2}$$\n",
    "$$X \\in \\mathbb{R}^{batch \\ size  \\ * \\  seq. \\ len. \\  * \\  embed. \\ dim.} $$  \n",
    "  \n",
    "2). **自注意力机制:**   \n",
    "$$Q = Linear(X) = XW_{Q}$$ \n",
    "$$K = Linear(X) = XW_{K} \\tag{eq.3}$$\n",
    "$$V = Linear(X) = XW_{V}$$\n",
    "$$X_{attention} = SelfAttention(Q, \\ K, \\ V) \\tag{eq.4}$$  \n",
    "  \n",
    "3). **残差连接与$Layer \\ Normalization$**\n",
    "$$X_{attention} = LayerNorm(X_{attention}) \\tag{eq. 5}$$\n",
    "$$X_{attention} = X + X_{attention} \\tag{eq. 6}$$  \n",
    "  \n",
    "4). **$FeedForward$，其实就是两层线性映射并用激活函数（比如说$ReLU$）激活：**   \n",
    "$$X_{hidden} = Linear(Activate(Linear(X_{attention}))) \\tag{eq. 7}$$  \n",
    "  \n",
    "5). **重复3).:**\n",
    "$$X_{hidden} = LayerNorm(X_{hidden})$$\n",
    "$$X_{hidden} = X_{attention} + X_{hidden}$$\n",
    "$$X_{hidden} \\in \\mathbb{R}^{batch \\ size  \\ * \\  seq. \\ len. \\  * \\  embed. \\ dim.} $$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Feed Forward$（前馈网络）层其实就是两层线性映射并用激活函数激活"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Encoder$ 由 $N=6$ 个相同的层组成。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Encoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_layers, num_heads, dff,\n",
    "                 input_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(Encoder, self).__init__()\n",
    "        self.num_layers = num_layers\n",
    "        self.d_model = d_model\n",
    "        self.emb = tf.keras.layers.Embedding(input_vocab_size, d_model)  # shape=[batch_size, seq_len, d_model]\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)  # shape=[1, max_seq_len, d_model]\n",
    "        self.encoder_layer = [EncoderLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                              for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def call(self, inputs, training, mask):\n",
    "        # 输入部分；inputs shape=[batch_size, seq_len]\n",
    "        seq_len = inputs.shape[1]  # 句子真实长度\n",
    "        word_embedding = self.emb(inputs)  # shape=[batch_size, seq_len, d_model]\n",
    "        word_embedding *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        emb= word_embedding + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x = self.encoder_layer[i](x, training, mask)\n",
    "        return x  # shape=[batch_size, seq_len, d_model]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 120, 512)\n"
     ]
    }
   ],
   "source": [
    "# 编码器测试\n",
    "sample_encoder = Encoder(512, 2, 8, 1024, 5000, 200)\n",
    "sample_encoder_output = sample_encoder(tf.random.uniform((64, 120)), False, None)\n",
    "print(sample_encoder_output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "每层 $Encoder\\ Block$ 都有两个子层组成。第一个子层实现了“多头”的 $Self\\text{-}attention$，第二个子层则是一个简单的 $Position\\text{-}wise$ 的全连接前馈网络。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class EncoderLayer(nn.Module):\n",
    "#     def __init__(self, size, self_attn, feed_forward, dropout):\n",
    "#         super(EncoderLayer, self).__init__()\n",
    "#         self.self_attn = self_attn\n",
    "#         self.feed_forward = feed_forward\n",
    "#         # SublayerConnection的作用就是把multi和ffn连在一起\n",
    "#         # 只不过每一层输出之后都要先做Layer Norm再残差连接\n",
    "#         self.sublayer = clones(SublayerConnection(size, dropout), 2)\n",
    "#         # d_model\n",
    "#         self.size = size\n",
    "\n",
    "#     def forward(self, x, mask):\n",
    "#         # 将embedding层进行Multi head Attention\n",
    "#         x = self.sublayer[0](x, lambda x: self.self_attn(x, x, x, mask))\n",
    "#         # 注意到attn得到的结果x直接作为了下一层的输入\n",
    "#         return self.sublayer[1](x, self.feed_forward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 三. 解码器部分（Decoder）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"./imgs/decoder.jpg\", width=550>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接着来看 $Decoder$ 部分（右半部分），它同样也是由 $N$ 层（在论文中，仍取 $N=6$ ）堆叠起来。  \n",
    "> 对于其中的每一层，除了与 $Encoder$ 中相同的 $self\\text{-}attention$ 及 $Feed Forward$ 两层之外，还在中间插入了一个传统的 $Encoder\\text{-}Decoder$ 框架中的 $context\\text{-}attention$ 层（上图中的$sub\\text{-}layer\\ 2$），即将 $Decoder$ 的输出作为 $query$ 去查询 $Encoder$ 的输出，同样用的是 $Multi\\text{-}Head\\ Attention$ ，使得在 $Decode$ 的时候能看到 $Encoder$ 的所有输出。  \n",
    "  \n",
    "这里明确一下 **$Decoder$ 的输入输出和解码过程：**\n",
    "\n",
    "- 输入：$Encoder$ 的输出 & 对应 $i-1$ 位置 $Decoder$ 的输出。所以中间的 $Attention$ 不是 $Self\\text{-}Attention$ ，它的 $K$，$V$ 来自 $Encoder$ ，Q来自上一位置 $Decoder$ 的输出\n",
    "- 输出：对应 $i$ 位置的输出词的概率分布\n",
    "- 解码：这里要特别注意一下，编码可以并行计算，一次性全部encoding出来，但解码不是一次把所有序列解出来的，而是像rnn一样一个一个解出来的，因为要用上一个位置的输入当作 $Attention$ 的 $query$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里 ** $Encoder$ 和 $Decoder$ 的 $Attention$ 的区别如下图所示**  \n",
    "<img src=\"./imgs/attention.png\", width=550>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DecoderLayer(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_heads, dff, dropout_rate=0.1):\n",
    "        super(DecoderLayer, self).__init__()\n",
    "        self.mha1 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.mha2 = MultiHeadAttention(d_model, num_heads)\n",
    "        self.ffn = point_wise_feed_forward(d_model, dff)\n",
    "        self.layernorm1 = LayerNormalization()\n",
    "        self.layernorm2 = LayerNormalization()\n",
    "        self.layernorm3 = LayerNormalization()\n",
    "        self.dropout1 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout2 = tf.keras.layers.Dropout(dropout_rate)\n",
    "        self.dropout3 = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def call(self, inputs, encoder_out, training, look_ahead_mask, padding_mask):\n",
    "#         tf.print(look_ahead_mask)\n",
    "        # masked multi-head attention: Q = K = V\n",
    "#         print('sassssssssssssssssssssssssssasssssssss')       \n",
    "        att_out1, att_weight1 = self.mha1(inputs, inputs, inputs, look_ahead_mask)\n",
    "#         print('sassssssssssssssssssssssssssasssssssss')\n",
    "#         tf.print(att_weight1)\n",
    "        att_out1 = self.dropout1(att_out1, training=training)\n",
    "        att_out1 = self.layernorm1(inputs + att_out1)\n",
    "        # multi-head attention: Q=att_out1, K = V = encoder_out\n",
    "        att_out2, att_weight2 = self.mha2(att_out1, encoder_out, encoder_out, padding_mask)\n",
    "        att_out2 = self.dropout2(att_out2, training=training)\n",
    "        att_out2 = self.layernorm2(att_out1 + att_out2)\n",
    "        # feed forward network\n",
    "        ffn_out = self.ffn(att_out2)\n",
    "        ffn_out = self.dropout3(ffn_out, training=training)\n",
    "        output = self.layernorm3(att_out2 + ffn_out)\n",
    "        return output, att_weight1, att_weight2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(tf.keras.layers.Layer):\n",
    "    def __init__(self, d_model, num_layers, num_heads, dff,\n",
    "                 target_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(Decoder, self).__init__()\n",
    "        self.seq_len = tf.shape\n",
    "        self.d_model = d_model\n",
    "        self.num_layers = num_layers\n",
    "        self.word_embedding = tf.keras.layers.Embedding(target_vocab_size, d_model)\n",
    "        self.pos_encoding = positional_encoding(max_seq_len, d_model)\n",
    "        self.decoder_layers = [DecoderLayer(d_model, num_heads, dff, dropout_rate)\n",
    "                               for _ in range(num_layers)]\n",
    "        self.dropout = tf.keras.layers.Dropout(dropout_rate)\n",
    "    def call(self, inputs, encoder_out, training, look_ahead_mask, padding_mask):\n",
    "#         tf.print(look_ahead_mask)\n",
    "#         tf.print(padding_mask)\n",
    "\n",
    "        seq_len = inputs.shape[1]\n",
    "        attention_weights = {}\n",
    "        word_embedding = self.word_embedding(inputs)\n",
    "        word_embedding *= tf.math.sqrt(tf.cast(self.d_model, tf.float32))\n",
    "        emb = word_embedding + self.pos_encoding[:, :seq_len, :]\n",
    "        x = self.dropout(emb, training=training)\n",
    "        for i in range(self.num_layers):\n",
    "            x, att1, att2 = self.decoder_layers[i](x, encoder_out, training,\n",
    "                                                   look_ahead_mask, padding_mask)\n",
    "            attention_weights['decoder_layer{}_att_w1'.format(i+1)] = att1\n",
    "            attention_weights['decoder_layer{}_att_w2'.format(i + 1)] = att2\n",
    "#             tf.print(att1)\n",
    "#             tf.print(att2)\n",
    "        return x, attention_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "明确了解码过程之后最上面的图就很好懂了，这里主要的不同就是新加的另外要说一下新加的attention多加了一个 $subsequent\\_mask$ ，因为训练时的output都是ground truth，这样可以确保预测第 $i$ 个位置时不会接触到未来的信息，具体解释如下。\n",
    "\n",
    "对于 $Encoder$ 中 $src$ 的 $mask$ 方式就比较简单，直接把 $pad$ 部分给 $mask$ 掉即可。  \n",
    "但对于 $Decoder$ 中 $trg$ 的 $mask$ 计算略微复杂一些，不仅需要把 $pad$ 部分 $mask$ 掉，还需要进行一个 $subsequent\\_mask$ 的操作。  \n",
    "即作为 $decoder$，在预测当前步的时候，是不能知道后面的内容的，即 $attention$ 需要加上 $mask$，将当前步之后的分数全部置为$-\\infty$，然后再计算 $softmax$，以防止发生数据泄露。这种 $Masked$ 的 $Attention$ 是考虑到输出 $Embedding$ 会偏移一个位置，确保了生成位置 $i$ 的预测时，仅依赖小于 $i$ 的位置处的已知输出，相当于把后面不该看到的信息屏蔽掉。  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'sample_encoder_output' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-22-cae99831a779>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0msample_decoder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mDecoder\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m512\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m2\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1024\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m5000\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m200\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m sample_decoder_output, attn = sample_decoder(tf.random.uniform((64, 100)),\n\u001b[1;32m----> 4\u001b[1;33m                                              sample_encoder_output, False, None, None)\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msample_decoder_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mattn\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'decoder_layer1_att_w2'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'sample_encoder_output' is not defined"
     ]
    }
   ],
   "source": [
    "# 解码器测试\n",
    "sample_decoder = Decoder(512, 2, 8, 1024, 5000, 200)\n",
    "sample_decoder_output, attn = sample_decoder(tf.random.uniform((64, 100)),\n",
    "                                             sample_encoder_output, False, None, None)\n",
    "print(sample_decoder_output.shape)\n",
    "print(attn['decoder_layer1_att_w2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_padding_mask(seq):\n",
    "    '''为了避免输入中padding的token对句子语义的影响，需要将padding位mask掉，\n",
    "    原来为0的padding项的mask输出为1; encoder和decoder过程都会用到'''\n",
    "    seq = tf.cast(tf.math.equal(seq, 0), tf.float32)\n",
    "    # 扩充维度以便于使用attention矩阵;seq输入shape=[batch_size, seq_len]；输出shape=[batch_siz, 1, 1, seq_len]\n",
    "    return seq[:, np.newaxis, np.newaxis, :]\n",
    "\n",
    "# look-ahead mask\n",
    "def create_look_ahead_mask(size):\n",
    "    '''用于对未预测的token进行掩码 这意味着要预测第三个单词，只会使用第一个和第二个单词。\n",
    "    要预测第四个单词，仅使用第一个，第二个和第三个单词，依此类推。只有decoder过程用到'''\n",
    "    # 产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # shape=[seq_len, seq_len]\n",
    "\n",
    "def create_mask(inputs, targets):\n",
    "    # 编码器只有padding_mask\n",
    "    encoder_padding_mask = create_padding_mask(inputs)\n",
    "    # 解码器decoder_padding_mask,用于第二层multi-head attention\n",
    "    decoder_padding_mask = create_padding_mask(inputs)\n",
    "    # seq_mask mask掉未预测的词\n",
    "    seq_mask = create_look_ahead_mask(tf.shape(targets)[1])\n",
    "    # decoder_targets_padding_mask 解码层的输入padding mask\n",
    "    decoder_targets_padding_mask = create_padding_mask(targets)\n",
    "    # 合并解码层mask，用于第一层masked multi-head attention\n",
    "    look_ahead_mask = tf.maximum(decoder_targets_padding_mask, seq_mask)\n",
    "    return encoder_padding_mask, look_ahead_mask, decoder_padding_mask"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "我们可视化一下 $subsequent\\_mask$ 矩阵的形式，直观进行理解。  \n",
    "这里的 $Attention mask$ 图显示了允许每个目标词（行）查看的位置（列）。在训练期间，当前解码位置的词不能 $Attend$ 到后续位置的词。  \n",
    "  \n",
    "> 这里是给定一个序列长度size，生成一个下三角矩阵，在主对角线右上的都是 False，其示意图如下：  \n",
    "<img src=\"./imgs/subsequent_mask.png\", width=450>  \n",
    "就是在decoder层的self-Attention中，由于生成 $s_i$ 时， $s_{i+1}$ 并没有产生，所以不能有 $s_i$ 和 $s_{i+1}$ 的关联系数，即只有下三角矩阵有系数，即下图中**`黄色部分`**。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(5,5))\n",
    "plt.imshow(subsequent_mask(20)[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 四. Transformer模型\n",
    "  \n",
    "最后，我们把 $Encoder$ 和 $Decoder$ 组成 $Transformer$ 模型"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Transformer包含编码器、解码器和最后的线性层，解码层的输出经过线性层后得到Transformer的输出'"
      ]
     },
     "execution_count": 130,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'''Transformer包含编码器、解码器和最后的线性层，解码层的输出经过线性层后得到Transformer的输出'''\n",
    "class Transformer(tf.keras.Model):\n",
    "    def __init__(self, d_model, num_layers, num_heads, dff,\n",
    "                 input_vocab_size, target_vocab_size, max_seq_len, dropout_rate=0.1):\n",
    "        super(Transformer, self).__init__()\n",
    "        self.encoder = Encoder(d_model, num_layers, num_heads, dff, input_vocab_size, max_seq_len, dropout_rate)\n",
    "        self.decoder = Decoder(d_model, num_layers, num_heads, dff, target_vocab_size, max_seq_len, dropout_rate)\n",
    "        self.final_layer = tf.keras.layers.Dense(target_vocab_size)\n",
    "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
    "             look_ahead_mask, decoder_padding_mask):\n",
    "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
    "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
    "#         tf.print(encoder_output)\n",
    "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
    "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
    "                                                   look_ahead_mask, decoder_padding_mask)\n",
    "        # 最后映射到输出层\n",
    "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
    "        return final_out, att_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformer测试\n",
    "sample_transformer = Transformer(\n",
    "num_layers=2, d_model=512, num_heads=8, dff=1024,\n",
    "input_vocab_size=8500, target_vocab_size=8000, max_seq_len=120\n",
    ")\n",
    "temp_input = tf.random.uniform((64, 62))\n",
    "temp_target = tf.random.uniform((64, 26))\n",
    "fn_out, att = sample_transformer(temp_input, temp_target, training=False,\n",
    "                              encoder_padding_mask=None,\n",
    "                               look_ahead_mask=None,\n",
    "                               decoder_padding_mask=None,\n",
    "                              )\n",
    "# print(fn_out.shape)\n",
    "# print(att['decoder_layer1_att_w1'].shape)\n",
    "# print(att['decoder_layer1_att_w2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# print(fn_out.shape)\n",
    "# print(att['decoder_layer1_att_w1'].shape)\n",
    "# print(att['decoder_layer1_att_w2'].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: id=658402, shape=(10, 10), dtype=float32, numpy=\n",
       "array([[0., 1., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 1., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 1., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 1., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 1., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 1., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 1., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 1., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 1.],\n",
       "       [0., 0., 0., 0., 0., 0., 0., 0., 0., 0.]], dtype=float32)>"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def create_look_ahead_mask(size):\n",
    "    '''用于对未预测的token进行掩码 这意味着要预测第三个单词，只会使用第一个和第二个单词。\n",
    "    要预测第四个单词，仅使用第一个，第二个和第三个单词，依此类推。只有decoder过程用到'''\n",
    "    # 产生一个上三角矩阵，上三角的值全为0。把这个矩阵作用在每一个序列上，就可以达到我们的目的。\n",
    "    mask = 1 - tf.linalg.band_part(tf.ones((size, size)), -1, 0)\n",
    "    return mask  # shape=[seq_len, seq_len]\n",
    "create_look_ahead_mask(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**定义设置超参并连接完整模型的函数**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 五. 模型训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**标签平滑**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在训练期间，我们采用了值$\\epsilon_{ls}=0.1$的标签平滑（参见： https://arxiv.org/pdf/1512.00567.pdf ），其实还是从$Computer\\; Vision$上搬过来的，具体操作可以看下面的代码实现，**在这里不作为重点**。  \n",
    "  \n",
    "这种做法提高了困惑度，因为模型变得更加不确定，但提高了准确性和BLEU分数。  \n",
    ">我们使用 $KL\\; div\\; loss$（KL散度损失）实现标签平滑。  \n",
    "对于输出的分布，从原始的 $one\\text{-}hot$ 分布转为在groundtruth上使用一个confidence值，而后其他的所有非groudtruth标签上采用 $\\frac{1 - confidence}{odim - 1}$ 作为概率值进行平滑。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LabelSmoothing(nn.Module):\n",
    "    \"\"\"标签平滑处理\"\"\"\n",
    "    def __init__(self, size, padding_idx, smoothing=0.0):\n",
    "        super(LabelSmoothing, self).__init__()\n",
    "        self.criterion = nn.KLDivLoss(reduction='sum')\n",
    "        self.padding_idx = padding_idx\n",
    "        self.confidence = 1.0 - smoothing\n",
    "        self.smoothing = smoothing\n",
    "        self.size = size\n",
    "        self.true_dist = None\n",
    "        \n",
    "    def forward(self, x, target):\n",
    "        assert x.size(1) == self.size\n",
    "        true_dist = x.data.clone()\n",
    "        true_dist.fill_(self.smoothing / (self.size - 2))\n",
    "        true_dist.scatter_(1, target.data.unsqueeze(1), self.confidence)\n",
    "        true_dist[:, self.padding_idx] = 0\n",
    "        mask = torch.nonzero(target.data == self.padding_idx)\n",
    "        if mask.dim() > 0:\n",
    "            true_dist.index_fill_(0, mask.squeeze(), 0.0)\n",
    "        self.true_dist = true_dist\n",
    "        return self.criterion(x, Variable(true_dist, requires_grad=False))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里的size是输出词表的大小，smoothing是用于分摊在非groundtruth上面的概率值。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这里，我们可以看到标签平滑的示例。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Label smoothing的例子\n",
    "crit = LabelSmoothing(5, 0, 0.4)  # 设定一个ϵ=0.4\n",
    "predict = torch.FloatTensor([[0, 0.2, 0.7, 0.1, 0],\n",
    "                             [0, 0.2, 0.7, 0.1, 0], \n",
    "                             [0, 0.2, 0.7, 0.1, 0]])\n",
    "v = crit(Variable(predict.log()), \n",
    "         Variable(torch.LongTensor([2, 1, 0])))\n",
    "\n",
    "# Show the target distributions expected by the system.\n",
    "print(crit.true_dist)\n",
    "plt.imshow(crit.true_dist)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如果对给定的选择非常有信心，标签平滑实际上会开始惩罚模型。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'LabelSmoothing' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-24-fb2f362e6d46>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mcrit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mLabelSmoothing\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m5\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0.1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m     \u001b[0md\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;36m3\u001b[0m \u001b[1;33m*\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m     \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFloatTensor\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mx\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m1\u001b[0m \u001b[1;33m/\u001b[0m \u001b[0md\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[1;31m#print(predict)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'LabelSmoothing' is not defined"
     ]
    }
   ],
   "source": [
    "crit = LabelSmoothing(5, 0, 0.1)\n",
    "def loss(x):\n",
    "    d = x + 3 * 1\n",
    "    predict = torch.FloatTensor([[0, x / d, 1 / d, 1 / d, 1 / d]])\n",
    "    #print(predict)\n",
    "    return crit(Variable(predict.log()), Variable(torch.LongTensor([1]))).item()\n",
    "\n",
    "plt.plot(np.arange(1, 100), [loss(x) for x in range(1, 100)])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**计算损失**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'由于目标序列是填充的，因此在计算损耗时应用填充掩码很重要。 padding的掩码为0，没padding的掩码为1'"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 损失和准确率\n",
    "'''由于目标序列是填充的，因此在计算损耗时应用填充掩码很重要。 padding的掩码为0，没padding的掩码为1'''\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True, reduction='none')\n",
    "def loss_fun(y_true, y_pred):\n",
    "    mask = tf.math.logical_not(tf.math.equal(y_true, 0))\n",
    "    loss_ = loss_object(y_true, y_pred)\n",
    "    mask = tf.cast(mask, dtype=loss_.dtype)\n",
    "    loss_ *= mask\n",
    "    return tf.reduce_mean(loss_)\n",
    "# 用于记录损失和准确率\n",
    "train_loss = tf.keras.metrics.Mean(name='train_loss')\n",
    "train_acc = tf.keras.metrics.SparseCategoricalAccuracy(name='train_accuracy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'shape'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-137-6ed107e6d186>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m \u001b[0mloss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mb\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m<ipython-input-136-46a0b937ec6c>\u001b[0m in \u001b[0;36mloss_fun\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mloss_fun\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlogical_not\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mequal\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m     \u001b[0mloss_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mloss_object\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m     \u001b[0mmask\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcast\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmask\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mloss_\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdtype\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m     \u001b[0mloss_\u001b[0m \u001b[1;33m*=\u001b[0m \u001b[0mmask\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, y_true, y_pred, sample_weight)\u001b[0m\n\u001b[0;32m    122\u001b[0m         y_true, y_pred, sample_weight)\n\u001b[0;32m    123\u001b[0m     \u001b[1;32mwith\u001b[0m \u001b[0mK\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mname_scope\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscope_name\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__class__\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__name__\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mgraph_ctx\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 124\u001b[1;33m       \u001b[0mlosses\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcall\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    125\u001b[0m       return losses_utils.compute_weighted_loss(\n\u001b[0;32m    126\u001b[0m           losses, sample_weight, reduction=self._get_reduction())\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, y_true, y_pred)\u001b[0m\n\u001b[0;32m    214\u001b[0m       \u001b[0mLoss\u001b[0m \u001b[0mvalues\u001b[0m \u001b[0mper\u001b[0m \u001b[0msample\u001b[0m\u001b[1;33m.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    215\u001b[0m     \"\"\"\n\u001b[1;32m--> 216\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_fn_kwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    217\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    218\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0mget_config\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\losses.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(y_true, y_pred, from_logits, axis)\u001b[0m\n\u001b[0;32m    971\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0msparse_categorical_crossentropy\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfrom_logits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    972\u001b[0m   return K.sparse_categorical_crossentropy(\n\u001b[1;32m--> 973\u001b[1;33m       y_true, y_pred, from_logits=from_logits, axis=axis)\n\u001b[0m\u001b[0;32m    974\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\keras\\backend.py\u001b[0m in \u001b[0;36msparse_categorical_crossentropy\u001b[1;34m(target, output, from_logits, axis)\u001b[0m\n\u001b[0;32m   4398\u001b[0m       \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mop\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4399\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 4400\u001b[1;33m   \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   4401\u001b[0m     \u001b[0moutput_rank\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   4402\u001b[0m   \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'shape'"
     ]
    }
   ],
   "source": [
    "a = [0,0,1,0]\n",
    "b = [1,0,0,0]\n",
    "loss_fun(a,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**optimizer优化器**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "论文里面提到了他们用的优化器，是以$\\beta_1=0.9、\\beta_2=0.98$ 和 $\\epsilon = 10^{−9}$ 的 $Adam$ 为基础，而后使用一种warmup的学习率调整方式来进行调节。  \n",
    "具体公式如下：  \n",
    "  \n",
    "$$ lrate = d^{−0.5}_{model}⋅min(step\\_num^{−0.5},\\; step\\_num⋅warmup\\_steps^{−1.5})$$  \n",
    "\n",
    "基本上就是用一个固定的 $warmup\\_steps$ **先进行学习率的线性增长（热身）**，而后到达 $warmup\\_steps$ 之后会随着 $step\\_num$ 的增长，以 $step\\_num$（步数）的反平方根成比例地**逐渐减小它**，他们用的 $warmup\\_steps = 4000$ ，这个可以针对不同的问题自己尝试。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'学习率衰减计算:lrate = d_model^-0.5 * min(step_num^-0.5, step_num*warmup_steps^-1.5)'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "# 动态学习率\n",
    "'''学习率衰减计算:lrate = d_model^-0.5 * min(step_num^-0.5, step_num*warmup_steps^-1.5)'''\n",
    "class CustomSchedule(tf.keras.optimizers.schedules.LearningRateSchedule):\n",
    "    def __init__(self, d_model, warmup_steps=4000):\n",
    "        super(CustomSchedule, self).__init__()\n",
    "        self.d_model = tf.cast(d_model, tf.float32)\n",
    "        self.warmup_steps = warmup_steps\n",
    "    def __call__(self, step):\n",
    "        step = tf.cast(step, tf.float32)\n",
    "        arg1 = tf.math.rsqrt(step)\n",
    "        arg2 = step * (self.warmup_steps ** -1.5)\n",
    "        return tf.math.rsqrt(self.d_model) * tf.math.minimum(arg1, arg2)\n",
    "\n",
    "# 学习率\n",
    "# lr = CustomSchedule(hp.d_model)\n",
    "\n",
    "        \n",
    "# def get_std_opt(model):\n",
    "#     return NoamOpt(model.src_embed[0].d_model, 2, 4000,\n",
    "#             torch.optim.Adam(model.parameters(), lr=0, betas=(0.9, 0.98), eps=1e-9))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "主要调节是在 $rate$ 这个函数中，其中\n",
    "- $model\\_size$ 即为 $d_{model}$\n",
    "- $warmup$ 即为 $warmup\\_steps$\n",
    "- $factor$ 可以理解为初始的学习率"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "以下对该优化器在**不同模型大小（$model\\_size$）**和**不同超参数（$marmup$）值**的情况下的学习率（$lrate$）曲线进行示例。 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Three settings of the lrate hyperparameters.\n",
    "opts = [CustomSchedule(512,  4000), \n",
    "        CustomSchedule(512,  8000),\n",
    "        CustomSchedule(256,  4000)]\n",
    "plt.plot(np.arange(1, 20000), [[opt(i) for opt in opts] for i in range(1, 20000)])\n",
    "plt.legend([\"512:4000\", \"512:8000\", \"256:4000\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 学习率\n",
    "lr = CustomSchedule(D_MODEL)\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-8)\n",
    "\n",
    "# temp_learing_rate = CustomSchedule(hp.d_model)\n",
    "# plt.plot(temp_learing_rate(tf.range(40000, dtype=tf.float32)))\n",
    "# plt.ylabel('learning rate')\n",
    "# plt.xlabel('train step')\n",
    "# plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**训练迭代**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "接下来，我们创建一个通用的训练和评分功能来跟踪损失。 我们传入一个上面定义的损失计算函数，它也处理参数更新。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\ntarget分为target_input和target real. target_input是传给解码器的输入，target_real是其左移一个位置的结果，\\n每个target_input位置对应下一个预测的标签\\n    如句子=“SOS A丛林中的狮子正在睡觉EOS”\\n    target_input =“SOS丛林中的狮子正在睡觉”\\n    target_real =“丛林中的狮子正在睡觉EOS”\\ntransformer是个自动回归模型：它一次预测一个部分，并使用其到目前为止的输出，决定下一步做什么。\\n在训练期间使用teacher-forcing，即无论模型当前输出什么都强制将正确输出传给下一步。\\n而预测时则根据前一个的输出预测下一个词\\n为防止模型在预期输出处达到峰值，模型使用look-ahead mask\\n'"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>>, which Python reported as:\n",
      "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
      "             look_ahead_mask, decoder_padding_mask):\n",
      "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
      "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
      "#         tf.print(encoder_output)\n",
      "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
      "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
      "                                                   look_ahead_mask, decoder_padding_mask)\n",
      "        # 最后映射到输出层\n",
      "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
      "        return final_out, att_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>>, which Python reported as:\n",
      "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
      "             look_ahead_mask, decoder_padding_mask):\n",
      "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
      "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
      "#         tf.print(encoder_output)\n",
      "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
      "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
      "                                                   look_ahead_mask, decoder_padding_mask)\n",
      "        # 最后映射到输出层\n",
      "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
      "        return final_out, att_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>>, which Python reported as:\n",
      "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
      "             look_ahead_mask, decoder_padding_mask):\n",
      "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
      "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
      "#         tf.print(encoder_output)\n",
      "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
      "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
      "                                                   look_ahead_mask, decoder_padding_mask)\n",
      "        # 最后映射到输出层\n",
      "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
      "        return final_out, att_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      "WARNING:tensorflow:Entity <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>>, which Python reported as:\n",
      "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
      "             look_ahead_mask, decoder_padding_mask):\n",
      "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
      "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
      "#         tf.print(encoder_output)\n",
      "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
      "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
      "                                                   look_ahead_mask, decoder_padding_mask)\n",
      "        # 最后映射到输出层\n",
      "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
      "        return final_out, att_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:Entity <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>>, which Python reported as:\n",
      "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
      "             look_ahead_mask, decoder_padding_mask):\n",
      "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
      "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
      "#         tf.print(encoder_output)\n",
      "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
      "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
      "                                                   look_ahead_mask, decoder_padding_mask)\n",
      "        # 最后映射到输出层\n",
      "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
      "        return final_out, att_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: Entity <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>> could not be transformed and will be executed as-is. Please report this to the AutoGraph team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output. Cause: Failed to parse source code of <bound method Transformer.call of <__main__.Transformer object at 0x000001D739B7F208>>, which Python reported as:\n",
      "    def call(self, inputs, targets, training, encoder_padding_mask,\n",
      "             look_ahead_mask, decoder_padding_mask):\n",
      "        # 首先encoder过程，输出shape=[batch_size, seq_len_input, d_model]\n",
      "        encoder_output = self.encoder(inputs, training, encoder_padding_mask)\n",
      "#         tf.print(encoder_output)\n",
      "        # 再进行decoder, 输出shape=[batch_size, seq_len_target, d_model]\n",
      "        decoder_output, att_weights = self.decoder(targets, encoder_output, training,\n",
      "                                                   look_ahead_mask, decoder_padding_mask)\n",
      "        # 最后映射到输出层\n",
      "        final_out = self.final_layer(decoder_output) # shape=[batch_size, seq_len_target, target_vocab_size]\n",
      "        return final_out, att_weights\n",
      "\n",
      "This may be caused by multiline strings or comments not indented at the same level as the code.\n",
      " epoch0,step:0, loss:1.0018, acc:0.0005\n",
      " epoch0,step:10, loss:0.9177, acc:0.0134\n",
      " epoch0,step:20, loss:0.9051, acc:0.0151\n",
      " epoch0,step:30, loss:0.9208, acc:0.0157\n",
      " epoch0,step:40, loss:0.9349, acc:0.0160\n",
      " epoch0,step:50, loss:0.9457, acc:0.0162\n",
      " epoch0,step:60, loss:0.9547, acc:0.0163\n",
      " epoch0,step:70, loss:0.9660, acc:0.0164\n",
      " epoch0,step:80, loss:0.9732, acc:0.0165\n",
      " epoch0,step:90, loss:0.9781, acc:0.0172\n",
      " epoch0,step:100, loss:0.9806, acc:0.0189\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-135-8d13567ff945>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     87\u001b[0m \u001b[1;31m#         print(inputs.eval())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;31m#         print(targets.eval())\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 89\u001b[1;33m         \u001b[0mtrain_step\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtargets\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     90\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mstep\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m10\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     91\u001b[0m             print(' epoch{},step:{}, loss:{:.4f}, acc:{:.4f}'.format(\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwds)\u001b[0m\n\u001b[0;32m    413\u001b[0m       \u001b[1;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    414\u001b[0m       \u001b[1;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 415\u001b[1;33m       \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=not-callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    416\u001b[0m     \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    417\u001b[0m       \u001b[1;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1820\u001b[0m     \u001b[1;34m\"\"\"Calls a graph function specialized to the inputs.\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1821\u001b[0m     \u001b[0mgraph_function\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1822\u001b[1;33m     \u001b[1;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m  \u001b[1;31m# pylint: disable=protected-access\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1823\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1824\u001b[0m   \u001b[1;33m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[1;34m(self, args, kwargs)\u001b[0m\n\u001b[0;32m   1139\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[0;32m   1140\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[1;32m-> 1141\u001b[1;33m         self.captured_inputs)\n\u001b[0m\u001b[0;32m   1142\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1143\u001b[0m   \u001b[1;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[1;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[0;32m   1222\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mexecuting_eagerly\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1223\u001b[0m       flat_outputs = forward_function.call(\n\u001b[1;32m-> 1224\u001b[1;33m           ctx, args, cancellation_manager=cancellation_manager)\n\u001b[0m\u001b[0;32m   1225\u001b[0m     \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1226\u001b[0m       \u001b[0mgradient_name\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_delayed_rewrite_functions\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mregister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\function.py\u001b[0m in \u001b[0;36mcall\u001b[1;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[0;32m    509\u001b[0m               \u001b[0minputs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    510\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"executor_type\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"config_proto\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 511\u001b[1;33m               ctx=ctx)\n\u001b[0m\u001b[0;32m    512\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    513\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[1;32mD:\\Program Files\\Anaconda\\lib\\site-packages\\tensorflow_core\\python\\eager\\execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[1;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[0;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[0;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 61\u001b[1;33m                                                num_outputs)\n\u001b[0m\u001b[0;32m     62\u001b[0m   \u001b[1;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     63\u001b[0m     \u001b[1;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "data = PrepareData(TRAIN_FILE, DEV_FILE)\n",
    "src_vocab = len(data.en_word_dict)\n",
    "tgt_vocab = len(data.cn_word_dict)\n",
    "print(\"src_vocab %d\" % src_vocab)\n",
    "print(\"tgt_vocab %d\" % tgt_vocab)\n",
    "# data = PrepareData(TRAIN_FILE, DEV_FILE)\n",
    "# input_vocab_size = len(data.en_word_dict)\n",
    "# target_vocab_size = len(data.cn_word_dict)\n",
    "# 学习率\n",
    "lr = CustomSchedule(D_MODEL)\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "transformer = Transformer(D_MODEL,\n",
    "                          LAYERS,\n",
    "                          H_NUM,\n",
    "                          D_FF,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          MAX_LENGTH,\n",
    "                          DROPOUT)\n",
    "\n",
    "# 创建checkpoint管理器\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "#                                           hp.ckpt_path,\n",
    "                                          './ckpt',\n",
    "                                          max_to_keep=3)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Load last checkpoint restore')\n",
    "\n",
    "\n",
    "'''\n",
    "target分为target_input和target real. target_input是传给解码器的输入，target_real是其左移一个位置的结果，\n",
    "每个target_input位置对应下一个预测的标签\n",
    "    如句子=“SOS A丛林中的狮子正在睡觉EOS”\n",
    "    target_input =“SOS丛林中的狮子正在睡觉”\n",
    "    target_real =“丛林中的狮子正在睡觉EOS”\n",
    "transformer是个自动回归模型：它一次预测一个部分，并使用其到目前为止的输出，决定下一步做什么。\n",
    "在训练期间使用teacher-forcing，即无论模型当前输出什么都强制将正确输出传给下一步。\n",
    "而预测时则根据前一个的输出预测下一个词\n",
    "为防止模型在预期输出处达到峰值，模型使用look-ahead mask\n",
    "'''\n",
    "@tf.function\n",
    "def train_step(inputs, targets):\n",
    "    tar_inp = targets[:, :-1]\n",
    "    tar_real = targets[:, 1:]\n",
    "#     print(tar_inp)\n",
    "#     print(tar_real)\n",
    "    # 构造mask\n",
    "    encoder_padding_mask, look_ahead_mask, decoder_padding_mask = create_mask(inputs, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred, _ = transformer(inputs,\n",
    "                              tar_inp,\n",
    "                              True,\n",
    "                              encoder_padding_mask,\n",
    "                              look_ahead_mask,\n",
    "                              decoder_padding_mask)\n",
    "#         np.array(pred.numpy)\n",
    "\n",
    "#         sess = tf.Session()\n",
    "        # Evaluate the tensor `c`.\n",
    "#         tf.print(pred)        \n",
    "#         np.array(pred)\n",
    "        loss = loss_fun(tar_real, pred)\n",
    "        # 求梯度\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        # 反向传播\n",
    "        optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        # 记录loss和acc\n",
    "        train_loss(loss)\n",
    "        train_acc(tar_real, pred)\n",
    "\n",
    "def run_epoch(data, model, loss_compute, epoch):\n",
    "    start = time.time()\n",
    "    total_tokens = 0.\n",
    "    total_loss = 0.\n",
    "    tokens = 0.\n",
    "\n",
    "    for i , batch in enumerate(data):\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)# 调用SimpleLossCompute\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Batch: %d Loss: %f Tokens per Sec: %fs\" % (epoch, i - 1, loss / batch.ntokens, (tokens.float() / elapsed / 1000.)))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "for epoch in range(EPOCHS):\n",
    "    start_time = time.time()\n",
    "    # 重置\n",
    "    train_loss.reset_states()\n",
    "    train_acc.reset_states()\n",
    "    for step, (inputs, targets) in enumerate(PrepareData(TRAIN_FILE, DEV_FILE).train_data):\n",
    "#         np.array(targets)\n",
    "#         print(inputs.eval())\n",
    "#         print(targets.eval())\n",
    "        train_step(inputs, targets)\n",
    "        if step % 10 == 0:\n",
    "            print(' epoch{},step:{}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "                epoch, step, train_loss.result(), train_acc.result()\n",
    "            ))\n",
    "    if epoch % 2 == 0:\n",
    "        ckpt_save_path = ckpt_manager.save()\n",
    "        print('epoch{}, save model at {}'.format(epoch, ckpt_save_path))\n",
    "    print('epoch:{}, loss:{:.4f}, acc:{:.4f}'.format(epoch, train_loss.result(), train_acc.result()))\n",
    "    print('time in one epoch:{}'.format(time.time() - start_time))\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(PrepareData(TRAIN_FILE, DEV_FILE).train_data):\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = PrepareData(TRAIN_FILE, DEV_FILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_vocab = len(data.en_word_dict)\n",
    "tgt_vocab = len(data.cn_word_dict)\n",
    "print(\"src_vocab %d\" % src_vocab)\n",
    "print(\"tgt_vocab %d\" % tgt_vocab)\n",
    "# data = PrepareData(TRAIN_FILE, DEV_FILE)\n",
    "# input_vocab_size = len(data.en_word_dict)\n",
    "# target_vocab_size = len(data.cn_word_dict)\n",
    "# 学习率\n",
    "lr = CustomSchedule(D_MODEL)\n",
    "# 优化器\n",
    "optimizer = tf.keras.optimizers.Adam(lr, beta_1=0.9, beta_2=0.98, epsilon=1e-9)\n",
    "\n",
    "transformer = Transformer(D_MODEL,\n",
    "                          LAYERS,\n",
    "                          H_NUM,\n",
    "                          D_FF,\n",
    "                          input_vocab_size,\n",
    "                          target_vocab_size,\n",
    "                          MAX_LENGTH,\n",
    "                          DROPOUT)\n",
    "\n",
    "# 创建checkpoint管理器\n",
    "ckpt = tf.train.Checkpoint(transformer=transformer,\n",
    "                           optimizer=optimizer)\n",
    "ckpt_manager = tf.train.CheckpointManager(ckpt,\n",
    "#                                           hp.ckpt_path,\n",
    "                                          './ckpt',\n",
    "                                          max_to_keep=3)\n",
    "if ckpt_manager.latest_checkpoint:\n",
    "    ckpt.restore(ckpt_manager.latest_checkpoint)\n",
    "    print('Load last checkpoint restore')\n",
    "\n",
    "\n",
    "'''\n",
    "target分为target_input和target real. target_input是传给解码器的输入，target_real是其左移一个位置的结果，\n",
    "每个target_input位置对应下一个预测的标签\n",
    "    如句子=“SOS A丛林中的狮子正在睡觉EOS”\n",
    "    target_input =“SOS丛林中的狮子正在睡觉”\n",
    "    target_real =“丛林中的狮子正在睡觉EOS”\n",
    "transformer是个自动回归模型：它一次预测一个部分，并使用其到目前为止的输出，决定下一步做什么。\n",
    "在训练期间使用teacher-forcing，即无论模型当前输出什么都强制将正确输出传给下一步。\n",
    "而预测时则根据前一个的输出预测下一个词\n",
    "为防止模型在预期输出处达到峰值，模型使用look-ahead mask\n",
    "'''\n",
    "@tf.function\n",
    "def train_step(inputs, targets, dev_set):\n",
    "    tar_inp = targets[:, :-1]\n",
    "    tar_real = targets[:, 1:]\n",
    "#     print(tar_inp)\n",
    "#     print(tar_real)\n",
    "    # 构造mask\n",
    "    encoder_padding_mask, look_ahead_mask, decoder_padding_mask = create_mask(inputs, tar_inp)\n",
    "\n",
    "    with tf.GradientTape() as tape:\n",
    "        pred, _ = transformer(inputs,\n",
    "                              tar_inp,\n",
    "                              True,\n",
    "                              encoder_padding_mask,\n",
    "                              look_ahead_mask,\n",
    "                              decoder_padding_mask)\n",
    "#         np.array(pred.numpy)\n",
    "\n",
    "#         sess = tf.Session()\n",
    "        # Evaluate the tensor `c`.\n",
    "#         tf.print(pred)        \n",
    "#         np.array(pred)\n",
    "        loss = loss_fun(tar_real, pred)\n",
    "        # 求梯度\n",
    "        gradients = tape.gradient(loss, transformer.trainable_variables)\n",
    "        # 反向传播\n",
    "        if dev_set == 1: \n",
    "            optimizer.apply_gradients(zip(gradients, transformer.trainable_variables))\n",
    "        # 记录loss和acc\n",
    "        train_loss(loss)\n",
    "        train_acc(tar_real, pred)\n",
    "\n",
    "def run_epoch(data, model, loss_compute, epoch):\n",
    "    start = time.time()\n",
    "    total_tokens = 0.\n",
    "    total_loss = 0.\n",
    "    tokens = 0.\n",
    "\n",
    "    for i , batch in enumerate(data):\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)# 调用SimpleLossCompute\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        tokens += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Batch: %d Loss: %f Tokens per Sec: %fs\" % (epoch, i - 1, loss / batch.ntokens, (tokens.float() / elapsed / 1000.)))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "for epoch in range(EPOCHS):\n",
    "    start = time.time()\n",
    "    total_tokens = 0.\n",
    "    total_loss = 0.\n",
    "    tokens = 0.\n",
    "    for i, (inputs, targets) in enumerate(data.train_data):\n",
    "#         np.array(targets)\n",
    "#         print(inputs.eval())\n",
    "#         print(targets.eval())\n",
    "        train_step(inputs, targets,1)\n",
    "        out = model(batch.src, batch.trg, batch.src_mask, batch.trg_mask)\n",
    "        loss = loss_compute(out, batch.trg_y, batch.ntokens)# 调用SimpleLossCompute\n",
    "\n",
    "        total_loss += loss\n",
    "        total_tokens += batch.ntokens\n",
    "        \n",
    "        += batch.ntokens\n",
    "\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Batch: %d Loss: %f Tokens per Sec: %fs\" % (epoch, i - 1, loss / batch.ntokens, (tokens.float() / elapsed / 1000.)))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "\n",
    "    return total_loss / total_tokens\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Batch: %d Loss: %f Tokens per Sec: %fs\" % (epoch, i - 1, loss / batch.ntokens, (tokens.float() / elapsed / 1000.)))\n",
    "            start = time.time()\n",
    "            tokens = 0\n",
    "    \n",
    "    for step, (inputs, targets) in enumerate(data.dev_data):\n",
    "#         np.array(targets)\n",
    "#         print(inputs.eval())\n",
    "#         print(targets.eval())\n",
    "        train_step(inputs, targets,0)\n",
    "#         if step % 10 == 0:\n",
    "#             print(' epoch{},step:{}, loss:{:.4f}, acc:{:.4f}'.format(\n",
    "#                 epoch, step, train_loss.result(), train_acc.result()\n",
    "#             ))\n",
    "#     if epoch % 2 == 0:\n",
    "#         ckpt_save_path = ckpt_manager.save()\n",
    "#         print('epoch{}, save model at {}'.format(epoch, ckpt_save_path))\n",
    "#     print('epoch:{}, loss:{:.4f}, acc:{:.4f}'.format(epoch, train_loss.result(), train_acc.result()))\n",
    "#     print('time in one epoch:{}'.format(time.time() - start_time))\n",
    "        if i % 50 == 1:\n",
    "            elapsed = time.time() - start\n",
    "            print(\"Epoch %d Batch: %d Loss: %f Tokens per Sec: %fs\" % (epoch, i - 1, loss / batch.ntokens, (tokens.float() / elapsed / 1000.)))\n",
    "            start = time.time()\n",
    "            tokens = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 六. 模型预测"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def greedy_decode(model, src, src_mask, max_len, start_symbol):\n",
    "    \"\"\"\n",
    "    传入一个训练好的模型，对指定数据进行预测\n",
    "    \"\"\"\n",
    "    # 先用encoder进行encode\n",
    "    memory = model.encode(src, src_mask)\n",
    "    # 初始化预测内容为1×1的tensor，填入开始符('BOS')的id，并将type设置为输入数据类型(LongTensor)\n",
    "    ys = torch.ones(1, 1).fill_(start_symbol).type_as(src.data)\n",
    "    # 遍历输出的长度下标\n",
    "    for i in range(max_len-1):\n",
    "        # decode得到隐层表示\n",
    "        out = model.decode(memory, \n",
    "                           src_mask, \n",
    "                           Variable(ys), \n",
    "                           Variable(subsequent_mask(ys.size(1)).type_as(src.data)))\n",
    "        # 将隐藏表示转为对词典各词的log_softmax概率分布表示\n",
    "        prob = model.generator(out[:, -1])\n",
    "        # 获取当前位置最大概率的预测词id\n",
    "        _, next_word = torch.max(prob, dim = 1)\n",
    "        next_word = next_word.data[0]\n",
    "        # 将当前位置预测的字符id与之前的预测内容拼接起来\n",
    "        ys = torch.cat([ys, \n",
    "                        torch.ones(1, 1).type_as(src.data).fill_(next_word)], dim=1)\n",
    "    return ys\n",
    "\n",
    "\n",
    "def evaluate(data, model):\n",
    "    \"\"\"\n",
    "    在data上用训练好的模型进行预测，打印模型翻译结果\n",
    "    \"\"\"\n",
    "    # 梯度清零\n",
    "    with torch.no_grad():\n",
    "        # 在data的英文数据长度上遍历下标\n",
    "        for i in range(len(data.dev_en)):\n",
    "            # TODO: 打印待翻译的英文句子\n",
    "            en_sent = \n",
    "            print(\"\\n\" + en_sent)\n",
    "            \n",
    "            # TODO: 打印对应的中文句子答案\n",
    "            cn_sent = \n",
    "            print(\"\".join(cn_sent))\n",
    "            \n",
    "            # 将当前以单词id表示的英文句子数据转为tensor，并放如DEVICE中\n",
    "            src = torch.from_numpy(np.array(data.dev_en[i])).long().to(DEVICE)\n",
    "            # 增加一维\n",
    "            src = src.unsqueeze(0)\n",
    "            # 设置attention mask\n",
    "            src_mask = (src != 0).unsqueeze(-2)\n",
    "            # 用训练好的模型进行decode预测\n",
    "            out = greedy_decode(model, src, src_mask, max_len=MAX_LENGTH, start_symbol=data.cn_word_dict[\"BOS\"])\n",
    "            # 初始化一个用于存放模型翻译结果句子单词的列表\n",
    "            translation = []\n",
    "            # 遍历翻译输出字符的下标（注意：开始符\"BOS\"的索引0不遍历）\n",
    "            for j in range(1, out.size(1)):\n",
    "                # 获取当前下标的输出字符\n",
    "                sym = data.cn_index_dict[out[0, j].item()]\n",
    "                # 如果输出字符不为'EOS'终止符，则添加到当前句子的翻译结果列表\n",
    "                if sym != 'EOS':\n",
    "                    translation.append(sym)\n",
    "                # 否则终止遍历\n",
    "                else:\n",
    "                    break\n",
    "            # 打印模型翻译输出的中文句子结果\n",
    "            print(\"translation: %s\" % \" \".join(translation))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 预测\n",
    "# 加载模型\n",
    "model.load_state_dict(torch.load(SAVE_FILE))\n",
    "# 开始预测\n",
    "print(\">>>>>>> start evaluate\")\n",
    "evaluate_start  = time.time()\n",
    "evaluate(data, model)         \n",
    "print(f\"<<<<<<< finished evaluate, cost {time.time()-evaluate_start:.4f} seconds\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": false,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": true,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "256px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
